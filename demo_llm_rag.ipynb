{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install the third party libraries for this project."
      ],
      "metadata": {
        "id": "EMY8atTfuQEM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9sJFJ2atkBP"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-core langchain-classic langchain_community langchain_text_splitters langchain_openai langgraph langsmith pydantic pypdf chromadb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary modules from the third party libraries for this project."
      ],
      "metadata": {
        "id": "nGIOVTclvgan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "import os\n",
        "\n",
        "# Load configuration from .env file\n",
        "loaded_env = load_dotenv(find_dotenv(), override=True)"
      ],
      "metadata": {
        "id": "cdDMG8zPuZv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test to ensure the LLM is working."
      ],
      "metadata": {
        "id": "KvRqJhdB5R95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.5)\n",
        "response = llm.invoke(\"Who is the president of America?\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "TY6O7SUl2IC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Context: This is external information provided to the LLM. It could be a document, a database record, a webpage, or any other source of facts. The LLM doesn't inherently \"know\" everything, so providing relevant context allows it to answer questions that are specific to that information.\n",
        "\n",
        "Instruction: These are the directives given to the LLM on how to process the context and answer the question. Instructions guide the model's behavior, such as \"summarize the following text,\" \"answer based only on the provided context,\" \"compare and contrast,\" or \"extract specific entities.\"\n",
        "\n",
        "Question: This is the specific query or task the user wants the LLM to address. The LLM uses the provided context and instructions to formulate an answer to this question.\n",
        "\n",
        "LangChain provides a structural way to call the LLM model."
      ],
      "metadata": {
        "id": "BFFDfeO5wVsP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "748eda44"
      },
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a friendly and helpful AI assistant who responds concisely.\"),\n",
        "    HumanMessage(content=\"Hello, how are you today?\")\n",
        "]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval-Augmented Generation (RAG) is an AI framework designed to enhance the accuracy and reliability of Large Language Models (LLMs) by grounding their responses in external, relevant information. Instead of relying solely on the LLM's pre-trained knowledge, RAG introduces a retrieval step where relevant documents or data (the context) are first fetched from a knowledge base. This context, along with specific directives (instructions) and the user's question, is then fed to the LLM. This process helps LLMs generate more factual, current, and contextually appropriate answers, significantly reducing the likelihood of 'hallucinations' and improving overall performance, especially for domain-specific or rapidly changing information."
      ],
      "metadata": {
        "id": "LhUbp5Hhxbr3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create two functions. First function for splitting the content, another function for loading into the vector store."
      ],
      "metadata": {
        "id": "Itb9lE3PxTt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_split_pdf(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Load a PDF file and split it into chunks using LangChain.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file\n",
        "        chunk_size (int): Size of each text chunk (default: 1000)\n",
        "        chunk_overlap (int): Overlap between chunks (default: 200)\n",
        "\n",
        "    Returns:\n",
        "        list: List of document chunks\n",
        "    \"\"\"\n",
        "    # Load PDF\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Initialize text splitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "    )\n",
        "\n",
        "    # Split documents into chunks\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def load_chunks_to_chroma(chunks, collection_name=\"pdf_documents\", persist_directory=\"./chroma_db\"):\n",
        "    \"\"\"\n",
        "    Load document chunks into ChromaDB vector store.\n",
        "\n",
        "    Args:\n",
        "        chunks (list): List of document chunks\n",
        "        collection_name (str): Name of the Chroma collection\n",
        "        persist_directory (str): Directory to save the Chroma database\n",
        "\n",
        "    Returns:\n",
        "        Chroma: Chroma vector store instance\n",
        "    \"\"\"\n",
        "\n",
        "    # OpenAIEmbeddings will automatically pick up the API key from the environment (OPENAI_API_KEY or OPENAI_APIKEY)\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "\n",
        "    # Create Chroma vector store from documents\n",
        "    vector_store = Chroma.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embeddings,\n",
        "        collection_name=collection_name,\n",
        "        persist_directory=persist_directory\n",
        "    )\n",
        "\n",
        "    return vector_store"
      ],
      "metadata": {
        "id": "xX4He3Dsu2NO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the PDF file and split it into text chunks. Then load the chunks into the vector store."
      ],
      "metadata": {
        "id": "VyRJCS8OxiOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "\n",
        "pdf_file = \"example.pdf\"  # Replace with your PDF file path\n",
        "vector_store = None\n",
        "\n",
        "# Delete the specific collection\n",
        "vector_db_directory = \"./chroma_db\"\n",
        "collection_name = \"pdf_documents\"\n",
        "\n",
        "if os.path.exists(vector_db_directory):\n",
        "    client = chromadb.PersistentClient(path=vector_db_directory)\n",
        "    try:\n",
        "        client.delete_collection(name=collection_name)\n",
        "        print(f\"Deleted collection '{collection_name}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Collection might not exist: {e}\")\n",
        "\n",
        "try:\n",
        "    # Load and split the PDF\n",
        "    text_chunks = load_and_split_pdf(pdf_file)\n",
        "\n",
        "    print(f\"Successfully loaded and split PDF into {len(text_chunks)} chunks\")\n",
        "\n",
        "    # Display all chunks\n",
        "    if text_chunks:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        for i, chunk in enumerate(text_chunks, 1):\n",
        "            print(f\"\\n--- CHUNK {i}/{len(text_chunks)} ---\")\n",
        "            print(f\"Page: {chunk.metadata.get('page', 'N/A')}\")\n",
        "            print(f\"Source: {chunk.metadata.get('source', 'N/A')}\")\n",
        "            print(f\"Content Length: {len(chunk.page_content)} characters\")\n",
        "            print(f\"\\nContent:\\n{chunk.page_content}\")\n",
        "            print(\"=\"*80)\n",
        "\n",
        "        # Load chunks into ChromaDB\n",
        "        print(\"\\n\\nLoading chunks into ChromaDB vector store...\")\n",
        "        vector_store = load_chunks_to_chroma(\n",
        "            chunks=text_chunks,\n",
        "            collection_name=collection_name,\n",
        "            persist_directory=vector_db_directory\n",
        "        )\n",
        "        print(f\"Successfully loaded {len(text_chunks)} chunks into ChromaDB!\")\n",
        "\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: PDF file '{pdf_file}' not found\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "tmOuygAKvsJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can query the vector database using natural language."
      ],
      "metadata": {
        "id": "bsy1mqtR0Jym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Perform a similarity search\n",
        "query = \"Where should I visit in China?\"  # Replace with your query\n",
        "results = vector_store.similarity_search(query, k=3)\n",
        "\n",
        "print(f\"\\nTop {len(results)} similar documents:\")\n",
        "for i, doc in enumerate(results, 1):\n",
        "    print(f\"\\n--- Result {i} ---\")\n",
        "    print(f\"Content: {doc.page_content}\")\n",
        "    print(f\"Metadata: {doc.metadata}\")"
      ],
      "metadata": {
        "id": "pInIa9oOxJf4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After retrieving from the vector database, we can pass the raw result to the LLM to retrieve the answer and rephrase it."
      ],
      "metadata": {
        "id": "NztfxEeS3WyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "You are an AI assistant helping answer questions based on the given context.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template\n",
        ")\n",
        "\n",
        "input_for_prompt = {\n",
        "    \"context\": results[0].page_content,\n",
        "    \"question\": query\n",
        "}\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "result = chain.invoke(input_for_prompt)\n",
        "\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "N85EJhHTzmrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A vector database is highly efficient at finding data points (like text chunks) that are numerically close to each other in an embedding space, which often correlates with semantic similarity. However, it does not comprehend or interpret the actual meaning of the text content itself. Its function is to identify patterns of similarity based on numerical representations, not to reason about or understand the information in a human-like cognitive sense. That deeper semantic understanding and reasoning is typically the role of a large language model (LLM) that processes the retrieved content.\n",
        "\n",
        "In the code below, we use LLM model to rerank the records retrieved from vector database."
      ],
      "metadata": {
        "id": "_VCbbx-iyhEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever(search_kwargs={ \"k\" : 3})\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
        "from langchain_classic.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.9)\n",
        "\n",
        "prompt_template = \"\"\"You are an AI assistant helping answer questions based on the given context.\n",
        "Use only the information provided in the context to answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template\n",
        ")\n",
        "\n",
        "# Create reranking compressor using the same LLM\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "\n",
        "# Wrap the existing retriever with the reranker\n",
        "rerank_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=retriever\n",
        ")\n",
        "\n",
        "def format_docs(docs):\n",
        "    \"\"\"Combine multiple document texts into a single string for the prompt.\"\"\"\n",
        "    combined_docs = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "    return combined_docs\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": rerank_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(rag_chain.invoke( \"What is the place of interest for France?\"))"
      ],
      "metadata": {
        "id": "y6xJ6VP4SR8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your turn to play with the rerank retriever."
      ],
      "metadata": {
        "id": "vq1iv2fjzwjJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d771520b"
      },
      "source": [
        "retriever = vector_store.as_retriever(search_kwargs={ \"k\" : 3})\n",
        "\n",
        "rerank_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=retriever\n",
        ")\n",
        "\n",
        "query = \"What should I eat in France?\"\n",
        "\n",
        "# Invoke the rerank_retriever directly\n",
        "retrieved_docs = rerank_retriever.invoke(query)\n",
        "\n",
        "print(f\"\\nRetrieved and re-ranked documents ({len(retrieved_docs)}):\\n\")\n",
        "for i, doc in enumerate(retrieved_docs, 1):\n",
        "    print(f\"--- Document {i} ---\")\n",
        "    print(f\"Content: {doc.page_content}\")\n",
        "    print(f\"Metadata: {doc.metadata}\")\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}