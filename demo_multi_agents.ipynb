{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8PYaVQYTLWV"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain_openai langchain_community langgraph-checkpoint-sqlite langgraph ipykernel python-dotenv deepagents deepeval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from deepagents import create_deep_agent\n",
        "from pprint import pprint\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    base_url=\"https://api.openai.com/v1\",\n",
        "    model=\"gpt-5.2\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=1000,\n",
        ")"
      ],
      "metadata": {
        "id": "GRAZmH4_T8RV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create two tools for the sub-agents."
      ],
      "metadata": {
        "id": "EGSAhHkcincY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "@tool(\n",
        "    \"get_risk_level\",\n",
        "    description=\"Given the account number, assess and return the risk level. Eg. Low, Medium or High.\"\n",
        ")\n",
        "def get_risk_level(account_number: str) -> str:\n",
        "  print(f\">>>>>>>>>>> Account number:{account_number}\")\n",
        "  return \"High risk\"\n",
        "\n",
        "@tool(\n",
        "    \"get_interest_rate\",\n",
        "    description=\"Given the risk level [Low, Medium or High], return the interest rate.\"\n",
        ")\n",
        "def get_interest_rate(risk_level: str) -> str:\n",
        "  print(f\">>>>>>>>>>> Risk Level:{risk_level}\")\n",
        "  if (risk_level.lower() == \"low\"):\n",
        "    return \"5%\"\n",
        "  elif (risk_level.lower() == \"medium\"):\n",
        "    return \"10%\"\n",
        "  elif (risk_level.lower() == \"high\"):\n",
        "    return \"15%\"\n",
        "  return \"20%\""
      ],
      "metadata": {
        "id": "C9RzOK1sVPKi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create two sub-agents. Risk Analyst to give the risk level of the applicant. And, Loan Officer to give the interest rate based on the risk level."
      ],
      "metadata": {
        "id": "Gg_-DNG0isdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "risk_analyst_subagent = {\n",
        "    \"name\": \"risk-analyst\",\n",
        "    \"description\": \"The purpose of this agent is to assess the risk of a loan application.\",\n",
        "    \"system_prompt\": \"\"\"You are a risk analyst that assess the risk level of loan application. Given the account number, use the tools provided to get the risk level.\"\"\",\n",
        "    \"tools\": [get_risk_level],\n",
        "}\n",
        "\n",
        "loan_officer_subagent = {\n",
        "    \"name\": \"loan-officer\",\n",
        "    \"description\": \"The purpose of this agent is to decide on the interest rate based on the risk level.\",\n",
        "    \"system_prompt\": \"\"\"You are a loan officer that decide on the interest rate based on the risk level of loan application.\"\"\",\n",
        "    \"tools\": [get_interest_rate],\n",
        "}\n",
        "\n",
        "subagents = [risk_analyst_subagent, loan_officer_subagent]"
      ],
      "metadata": {
        "id": "cBy6vTODUKkL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the supervisor or main agent. Assign both sub-agents to the supervisor agent."
      ],
      "metadata": {
        "id": "4cr-g7a6i8qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_officer_agent = create_deep_agent(\n",
        "    model=llm,\n",
        "    system_prompt=\"\"\"\n",
        "      You are bank customer officer that handles loan application.\n",
        "      The following agents are also involved in processing loan application.\n",
        "      1. risk-analyst - The purpose of this agent is to assess the risk of a loan application.\n",
        "      2. loan-officer - The purpose of this agent is to decide on the interest rate based on the risk level.\n",
        "      The business process for assessing risk and deciding interest rate STRICTLY follows the steps below:\n",
        "      1. Given the account number, check with risk-analyst to get the risk level.\n",
        "      2. Use the risk level to check with loan-officer to decide on the interest rate.\n",
        "      3. Return the interest rate.\n",
        "    \"\"\",\n",
        "    subagents=subagents\n",
        ")"
      ],
      "metadata": {
        "id": "TWjQ3HDii5BJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can test the multi-agents application."
      ],
      "metadata": {
        "id": "GTpWqv65i8MB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "last_message = None\n",
        "\n",
        "messages = []\n",
        "\n",
        "messages.append(\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Account number 123 would like to take a loan.\",\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"--- Starting Agent Stream ---\")\n",
        "\n",
        "# Keep track of the number of messages already printed\n",
        "seen_messages_count = 0\n",
        "\n",
        "for chunk in customer_officer_agent.stream(\n",
        "    {\"messages\": messages},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    if \"messages\" in chunk:\n",
        "        # Iterate only over new messages\n",
        "        for message in chunk[\"messages\"][seen_messages_count:]:\n",
        "            print(f\"\\n---\")\n",
        "            if message.content:\n",
        "                print(f\"Role: {message.type.capitalize()}\\nContent: {message.content}\")\n",
        "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
        "                print(f\"Tool Calls: {message.tool_calls}\")\n",
        "            if hasattr(message, 'tool_output') and message.tool_output:\n",
        "                print(f\"Tool Output: {message.tool_output}\")\n",
        "            if hasattr(message, 'name') and message.name:\n",
        "                print(f\"Name: {message.name}\")\n",
        "            last_message = message\n",
        "        seen_messages_count = len(chunk[\"messages\"])\n",
        "\n",
        "print(\"\\n--- Agent Stream Finished ---\")\n",
        "\n",
        "# print(f\"AI:{last_message}\") # This line is commented out as the detailed output is printed above\n"
      ],
      "metadata": {
        "id": "UrgW6PcrXmhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traditional unit testing for normal application cannot be used for agentic AI. To test the output of agent, the testing tool needs to understand the context of the output. Therefore, we need to use LLM to test the agent."
      ],
      "metadata": {
        "id": "k5CwGvRw9-p8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import (\n",
        "    GEval,\n",
        "    TaskCompletionMetric,\n",
        "    AnswerRelevancyMetric\n",
        ")\n",
        "from deepeval.dataset import EvaluationDataset, Golden\n",
        "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
        "from deepeval import evaluate\n",
        "\n",
        "answer_relevancy = AnswerRelevancyMetric(threshold=0.8, model=\"gpt-4o-mini\")\n",
        "\n",
        "# Custom accuracy metric\n",
        "accuracy_metric = GEval(\n",
        "    name=\"Loan Assessment Accuracy\",\n",
        "    criteria=\"The output correctly identifies the customer, retrieves accurate credit score, calculates appropriate risk level, and provides the correct interest rate\",\n",
        "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
        "    threshold=0.8,\n",
        "    model=\"gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# Create test cases list to collect all evaluations\n",
        "test_cases = []\n",
        "\n",
        "dataset = EvaluationDataset(\n",
        "    goldens=[\n",
        "        Golden(\n",
        "            input=\"Account number 123 would like to take a loan.\",\n",
        "            expected_output=\"For account number 123, the applicable annual interest rate for the new loan is: 15%\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def run_agent_and_get_output(agent_instance, user_input):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_input,\n",
        "        }\n",
        "    ]\n",
        "    last_message_content = \"\"\n",
        "    for chunk in agent_instance.stream(\n",
        "        {\"messages\": messages},\n",
        "        stream_mode=\"values\",\n",
        "    ):\n",
        "        if \"messages\" in chunk:\n",
        "            for message in chunk[\"messages\"]:\n",
        "                if message.content:\n",
        "                    last_message_content = message.content\n",
        "    return last_message_content\n",
        "\n",
        "# Build test cases from dataset\n",
        "for golden in dataset.goldens:\n",
        "    # Run agent and get response\n",
        "    response = run_agent_and_get_output(customer_officer_agent, golden.input)\n",
        "\n",
        "    # Create test case\n",
        "    test_case = LLMTestCase(\n",
        "        input=golden.input,\n",
        "        actual_output=response,\n",
        "        expected_output=golden.expected_output\n",
        "    )\n",
        "\n",
        "    test_cases.append(test_case)\n",
        "\n",
        "# NOW EVALUATE\n",
        "evaluate(test_cases, metrics=[accuracy_metric, answer_relevancy])\n"
      ],
      "metadata": {
        "id": "CXQv3byh7UsI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}